{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fca8e8262f1d4628bb96eba727159757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_04b77020d572443bab7c03bc36658a91",
              "IPY_MODEL_53f2e5a860494b0b9b39f458b13df955",
              "IPY_MODEL_5034b91d2957458db244dafda74a9f42"
            ],
            "layout": "IPY_MODEL_8c5a855be13b4de4b642be45f5ae8ac3"
          }
        },
        "04b77020d572443bab7c03bc36658a91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_835ec76e618540989e1cdaa8e7e88215",
            "placeholder": "​",
            "style": "IPY_MODEL_3c7c3e497e8945aaa2c1274eacf0c0aa",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "53f2e5a860494b0b9b39f458b13df955": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_451a73c464a94ad99759d031730c149e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3d8d2f139484436a8b29ed75084de4a2",
            "value": 2
          }
        },
        "5034b91d2957458db244dafda74a9f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1e950e40c794d97889066305791a251",
            "placeholder": "​",
            "style": "IPY_MODEL_bb73d5a054e64e528276088ab26636ab",
            "value": " 2/2 [00:31&lt;00:00, 15.07s/it]"
          }
        },
        "8c5a855be13b4de4b642be45f5ae8ac3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "835ec76e618540989e1cdaa8e7e88215": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c7c3e497e8945aaa2c1274eacf0c0aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "451a73c464a94ad99759d031730c149e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d8d2f139484436a8b29ed75084de4a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1e950e40c794d97889066305791a251": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb73d5a054e64e528276088ab26636ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview of Transformer Models**\n",
        "\n",
        "### **1. Text-In, Text-Out Nature of Transformers**\n",
        "\n",
        "Transformer LLMs are designed as **text-in, text-out systems**. You give them a text prompt, and they produce a text response.\n",
        "**Example:**\n",
        "Input – “Write an email apologizing for a late delivery.”\n",
        "Output – “Dear Customer, we sincerely apologize for the delay in your order...”\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Generation Happens One Token at a Time**\n",
        "\n",
        "Instead of generating the entire text in one go, the model predicts **one token (a word or subword)** at each step. Each token is produced through a **forward pass**—a single computation through the neural network.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Autoregressive Nature of LLMs**\n",
        "\n",
        "Transformer LLMs are **autoregressive**, meaning they use their **own previous predictions** to generate the next token.\n",
        "**Example:**\n",
        "After generating “Dear,” the model appends it to the prompt and uses it to predict the next token, “Customer,” and so on.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Loop-Based Generation Mechanism**\n",
        "\n",
        "The process runs in a **loop**:\n",
        "\n",
        "1. Take the current prompt.\n",
        "2. Predict the next token.\n",
        "3. Append the new token to the prompt.\n",
        "4. Repeat until an end-of-text signal is reached.\n",
        "\n",
        "This mechanism allows the model to generate entire paragraphs sequentially.\n",
        "\n",
        "**Visual Description:**\n",
        "\n",
        "```\n",
        "Input Prompt → Generate Token → Append Token to Prompt → Repeat\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Importance of Large-Scale Training**\n",
        "\n",
        "Transformers produce impressive and contextually accurate outputs because they are trained on **massive, high-quality datasets**. The scale of the model and data is a major reason behind their success in real-world applications like chatbots, summarization tools, and code assistants.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Difference Between Generative and Representation Models**\n",
        "\n",
        "Autoregressive models like GPT focus on **generating text sequentially**. In contrast, **representation models** like BERT focus on **understanding text** by analyzing context but do not generate text token by token.\n",
        "\n",
        "* **Autoregressive:** Used for chatbots, content creation, and coding assistants.\n",
        "* **Representation Models:** Used for tasks like search ranking, sentiment analysis, and question answering.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Foundation of Modern LLMs**\n",
        "\n",
        "The Transformer architecture, introduced in 2017, forms the **foundation of most modern LLMs** (like GPT, Claude, and Gemini). Later improvements built upon this architecture to enhance speed, scale, and accuracy.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gatoh__p2kTo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-pRksjF1PcR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (AutoModelForCausalLM, AutoTokenizer,AutoModel,\n",
        "pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI08rEhS3iho",
        "outputId": "1a59f6a3-ab07-47a5-bbea-bc07192b393d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "\"microsoft/Phi-3-mini-4k-instruct\",\n",
        "device_map=\"cuda\",\n",
        "torch_dtype=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "fca8e8262f1d4628bb96eba727159757",
            "04b77020d572443bab7c03bc36658a91",
            "53f2e5a860494b0b9b39f458b13df955",
            "5034b91d2957458db244dafda74a9f42",
            "8c5a855be13b4de4b642be45f5ae8ac3",
            "835ec76e618540989e1cdaa8e7e88215",
            "3c7c3e497e8945aaa2c1274eacf0c0aa",
            "451a73c464a94ad99759d031730c149e",
            "3d8d2f139484436a8b29ed75084de4a2",
            "a1e950e40c794d97889066305791a251",
            "bb73d5a054e64e528276088ab26636ab"
          ]
        },
        "id": "0ilupxa035tq",
        "outputId": "39bbf636-2626-49b1-93b5-07b014b20403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fca8e8262f1d4628bb96eba727159757"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline\n",
        "generator = pipeline(\n",
        "\"text-generation\",\n",
        "model=model,\n",
        "tokenizer=tokenizer,\n",
        "return_full_text=True,\n",
        "max_new_tokens=50,\n",
        "do_sample=False\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-_SJjne4BtC",
        "outputId": "893fa5a8-6e33-4e8d-9ea1-20f260ff954e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "output = generator(prompt)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bY7MgQUA2bi",
        "outputId": "d0dffcc6-b13b-4e32-e6a1-4b3a85b7fd2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened. Mention the steps you're taking to prevent it in the future.\n",
            "\n",
            "Dear Sarah,\n",
            "\n",
            "I hope this message finds you well. I am writing to express my sincerest apologies for the unfortunate incident that occurred\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Elaborated Explanation: Components of the Forward Pass**\n",
        "\n",
        "The forward pass in a Transformer-based LLM describes **how the model processes input text and predicts the next token step by step**. It consists of three major parts: the **Tokenizer**, the **Transformer stack**, and the **Language Modeling Head (LM head)**.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Tokenizer – Converting Text into Token IDs**\n",
        "\n",
        "* **Purpose:** Converts raw human-readable text into a sequence of **token IDs** the model can understand.\n",
        "* **Vocabulary & Embeddings:**\n",
        "  The tokenizer has a fixed **vocabulary table** (e.g., 50,000 tokens). Each token has a corresponding **embedding vector**, a numerical representation that encodes semantic meaning.\n",
        "* **Example:**\n",
        "  Input text `\"The cat sat\"` might become `[101, 452, 678]`, where each number corresponds to a learned vector.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Transformer Block Stack – Core Processing Engine**\n",
        "\n",
        "After tokenization, the embeddings pass through a **stack of Transformer decoder layers** (e.g., 32 layers in the Phi3 model). Each block contains several sub-components:\n",
        "\n",
        "* **Self-Attention Layer:**\n",
        "\n",
        "  * Learns **relationships between tokens** regardless of their position.\n",
        "  * Example: It helps the model understand that in “The cat sat on the mat,” the word **“sat”** is related to **“cat”**.\n",
        "  * Attention uses **query, key, value projections** internally to decide which tokens influence each other most.\n",
        "\n",
        "* **Feedforward Network (MLP):**\n",
        "\n",
        "  * Applies **non-linear transformations** to enhance features learned by attention.\n",
        "  * Expands and contracts vector dimensions to better represent context.\n",
        "\n",
        "* **Normalization & Dropout:**\n",
        "\n",
        "  * **Layer normalization** stabilizes training and improves performance.\n",
        "  * **Dropout** randomly drops some connections during training to prevent overfitting (though often inactive during inference).\n",
        "\n",
        "* **Stacking Blocks:**\n",
        "\n",
        "  * The model stacks multiple identical decoder layers so the text representation becomes increasingly context-aware as it moves up the stack.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Language Modeling Head (LM Head) – Predicting the Next Token**\n",
        "\n",
        "* After the Transformer blocks finish processing, the output is passed to the **LM head**, which is typically a **linear layer**.\n",
        "* **Purpose:** Converts the contextual vector into a **probability distribution over the entire vocabulary**.\n",
        "* **Example:**\n",
        "  For the input “The cat,” the LM head might output probabilities like:\n",
        "\n",
        "  * sat: 0.65\n",
        "  * ran: 0.15\n",
        "  * slept: 0.10\n",
        "  * (others: smaller values)\n",
        "    The model picks **“sat”** as it has the highest probability.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Forward Pass for Each Token (Autoregressive Loop)**\n",
        "\n",
        "* The above process happens **once per generated token**.\n",
        "* After predicting a token (e.g., “sat”), it is **appended to the prompt**, and the forward pass runs again to predict the next token.\n",
        "* This is called **autoregressive generation**, where **previous outputs influence future predictions**.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Other Heads Beyond LM Head**\n",
        "\n",
        "* While text generation uses an LM head, the same Transformer stack can be connected to **other heads** for different tasks:\n",
        "\n",
        "  * **Sequence Classification Head:** Used for sentiment analysis or spam detection.\n",
        "  * **Token Classification Head:** Used for tasks like named-entity recognition (NER).\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Example – Phi3 Model Highlights**\n",
        "\n",
        "* Embedding matrix: **32,064 tokens**, each represented by a **3,072-dimensional vector**.\n",
        "* Stack: **32 Transformer decoder layers**, each with attention + MLP.\n",
        "* LM head: Takes **3,072-dimensional vectors** and outputs probabilities across the **entire vocabulary**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Markdown-Friendly Forward Pass Diagram**\n",
        "\n",
        "Paste the following into a **Colab or Jupyter Markdown cell**:\n",
        "\n",
        "<pre>\n",
        "Input Text (e.g., \"The cat sat\")\n",
        "    │\n",
        "    ▼\n",
        "┌──────────────┐\n",
        "│  Tokenizer   │ → Converts text to token IDs & embeddings (vector form)\n",
        "└──────────────┘\n",
        "    │\n",
        "    ▼\n",
        "┌─────────────────────────────┐\n",
        "│  Transformer Block Stack    │  (e.g., 32 decoder layers)\n",
        "│  ┌───────────────────────┐ │\n",
        "│  │ Self-Attention Layer  │ │ → Learns token relationships & context\n",
        "│  │ Feedforward (MLP)     │ │ → Non-linear feature transformations\n",
        "│  │ Norm + Dropout        │ │ → Stabilization & regularization\n",
        "│  └───────────────────────┘ │\n",
        "└─────────────────────────────┘\n",
        "    │\n",
        "    ▼\n",
        "┌──────────────┐\n",
        "│   LM Head    │ → Converts final vector to probability scores over vocabulary\n",
        "└──────────────┘\n",
        "    │\n",
        "    ▼\n",
        "Select most likely token → Append to input → Repeat for next token\n",
        "</pre>\n",
        "\n",
        "---\n",
        "\n",
        "### **Why This Matters**\n",
        "\n",
        "* **Tokenizer:** Translates human language into machine-readable IDs.\n",
        "* **Transformer Stack:** Understands meaning, context, and relationships.\n",
        "* **LM Head:** Predicts and generates coherent text.\n",
        "* **Autoregressive Loop:** Allows long, context-aware text generation.\n",
        "\n"
      ],
      "metadata": {
        "id": "yiNirx0o-wZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input prompt\n",
        "prompt = \"The capital of Germany is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to('cuda')"
      ],
      "metadata": {
        "id": "XZgxKQjfK94P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbHB5qGi7KkL",
        "outputId": "a962c30b-7021-41a4-8280-5d9faccaf47c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 450, 7483,  310, 9556,  338]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_outputs = model(inputs,output_hidden_states=True, return_dict=True)\n",
        "last_output = all_outputs.hidden_states[-1]"
      ],
      "metadata": {
        "id": "imb7pOT7EapO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpnR_WEHR2wC",
        "outputId": "5bc847da-8e88-4204-8f6c-7769e43639ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0nZjS_mUy3b",
        "outputId": "148fbb42-e7aa-4982-fc02-fd161f4695f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.3047,  1.1953,  0.2988,  ..., -0.3008,  0.6758,  0.1406],\n",
              "         [-0.1318,  0.3320,  0.3906,  ...,  0.5703, -0.1494, -0.6172],\n",
              "         [-0.5781,  1.0781,  1.5469,  ..., -0.4121,  0.2871,  0.3906],\n",
              "         [-0.4297,  0.8594,  0.2061,  ...,  0.0605,  0.1260, -0.1484],\n",
              "         [-1.0078, -0.2910,  0.3184,  ...,  0.5938,  0.6484, -0.8242]]],\n",
              "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head_output = all_outputs.logits"
      ],
      "metadata": {
        "id": "RyxIpixrr0rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm_head_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odX86jCMsBVD",
        "outputId": "6097bafb-ebc0-4f08-d658-5411f108f7b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 32064])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_token = lm_head_output[0, -1, :].argmax().item()\n",
        "next_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnZZh-PesSTl",
        "outputId": "1dece2c6-db25-4212-8161-389f8088610c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5115"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Next Word: ',tokenizer.decode(next_token))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzGxMgHwsi8-",
        "outputId": "18d3dbd8-d9a2-447f-cb0d-53786f9ee87a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next Word:  Berlin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s the summarized explanation **with a simple diagram you can paste directly in Colab (Markdown)** to illustrate **KV caching vs no caching**:\n",
        "\n",
        "---\n",
        "\n",
        "### **Speeding Up Generation with KV Caching**\n",
        "\n",
        "* **Problem During Generation:**\n",
        "  Each new token generation traditionally reprocesses all previous tokens, causing redundant computations.\n",
        "\n",
        "* **Solution – KV Cache:**\n",
        "\n",
        "  * **Keys & Values (K/V):** Important components in the attention mechanism.\n",
        "  * **Caching:** Store computed K/V vectors from earlier tokens.\n",
        "  * **Result:** On subsequent steps, only computations for the **new token** are done; cached results are reused.\n",
        "  * **Benefit:** Significant speedup during text generation.\n",
        "\n",
        "* **Implementation in Hugging Face:**\n",
        "\n",
        "  * KV caching is **enabled by default**.\n",
        "  * Disable using `use_cache=False`.\n",
        "  * Speedup is noticeable especially for long text generation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Diagram – Without KV Caching**\n",
        "\n",
        "Step 1: [Token 1] ──> [Model Forward Pass] ──> [Output Token 1]\n",
        "\n",
        "Step 2: [Token 1, Token 2] ──> [Model Forward Pass] ──> [Output Token 2]\n",
        "\n",
        "Step 3: [Token 1, Token 2, Token 3] ──> [Model Forward Pass] ──> [Output Token 3]\n",
        "\n",
        "(Every step reprocesses ALL tokens)\n",
        "\n",
        "### **With KV Cache**\n",
        "\n",
        "Step 1: [Token 1] ──> [Model Forward Pass + Cache K/V] ──> [Output Token 1]\n",
        "\n",
        "Step 2: [Token 2] ──> [Use Cached K/V + Compute Only New Token] ──> [Output Token 2]\n",
        "\n",
        "Step 3: [Token 3] ──> [Use Cached K/V + Compute Only New Token] ──> [Output Token 3]\n",
        "\n",
        "(Reuse cached computations; only new token is processed)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZU8zPYhk-HTw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCpQsb0o0CX-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}